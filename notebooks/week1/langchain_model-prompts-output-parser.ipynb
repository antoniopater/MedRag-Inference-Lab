{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde8a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n",
    "!pip install google-generativeai\n",
    "!pip install langchain-google-genai\n",
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3094fae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) #find_dotenv() is used to find the .env file\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34292784",
   "metadata": {},
   "source": [
    "### Jaka jest róznica miedzy zwyklym modelem LLM a modelem typu Chat Model?\n",
    "- LLM przyjmuje jeden tekst i zwraca tekst. Piszesz -> odpsiuje \n",
    "- Chat Model przyjmuje listę wiadomości. Kada wiadomośc ma swoją rolę (system, user, assistant) To jak rozmoa na czacie.\n",
    "\n",
    "Stosuje się dwa rózne importy przez to:\n",
    "- LLM `from langchain.llms import OpenAI`\n",
    "- ChatModel `from langchain.chat_models import ChatOpenAI`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86ed3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gemini-2.5-flash\"):\n",
    "    model_instance = genai.GenerativeModel(model)\n",
    "    response = model_instance.generate_content(\n",
    "        prompt,\n",
    "        generation_config=genai.types.GenerationConfig(temperature=0)\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b2c85b",
   "metadata": {},
   "source": [
    "- `temperature` - to parametr \"kreatywności\":\n",
    "    - 0 oznacza, ze model jest deterministyczny\n",
    "    - wyzsze wartosci np. 0.7 sprawilby ze model bylby bardziej fantazyjny\n",
    "    \n",
    "- `response.text` - odpowiedz z Gemini API. Model zwraca obiekt response, z którego wyciagamy tekst przez atrybut `.text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b27742",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3879f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1488d85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c41f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa9e2db",
   "metadata": {},
   "source": [
    "### ChatAPI : LangChain - Gemini approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e4129e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.0)\n",
    "chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf603e",
   "metadata": {},
   "source": [
    "Uywamy `ChatPromptTemplate` aby tworzyc formularz dla modelu\n",
    "1. `template_string` - ot makieta, nawiasy klamrowe {style} i {text} - langchain rozpozna je jako puste pola do wypelnienia\n",
    "2. `ChatPromptTemplate.from_template(...)` - metoda zamienia zwykły tekst w inteligentny obiekt LangChain. Ten obiekt wie jakie ma zmienne i jak przygotowac je do modelu\n",
    "3. `prompt.format(...)` - wstrzykujemy konkretne dane do formularza. Operacja ta to wciaz tekst gotowy od wyslania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272cd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template_string)\n",
    "prompt.format(style=\"American English\", text=\"I'm very happy with the product\")\n",
    "\n",
    "print(type(prompt))\n",
    "print(type(prompt[0]))\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424d8753",
   "metadata": {},
   "source": [
    "Dlaczego `prompt[0]` i typy sa takie wazne?\n",
    "\n",
    "Kiedy sprawdzamy type(prompt) zobaczymy ze to nie jest zwykly `str` \n",
    "\n",
    "`type(prompt)` - to struktura, która moze skladac sie z wielu wiadomosci (System. Human, AI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cb2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c82a730",
   "metadata": {},
   "source": [
    "Hierarchia obiektu `PromptTemplate`\n",
    "\n",
    "`prompt` : `ChatPromptTemplate` - caly szablon-matka. zawiera liste wiadomosci, definicje zmiennych i instrukcje formatowania\n",
    "\n",
    "`prompt.messages` : `list` - lista wszystkich czesci skladowych Twojego promptu. Nawet jesli masz tylko ejden tekst, on trafia na te liste\n",
    "\n",
    "`prompt.messages[0]`: `HumanMessagePromptTemplate` - pierwszy klocek na liscie. Langchatin domyslnie zaklada, ze szablon tektstowy to wiadomosc od czlowieka\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb777c",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string_1 = \"Przetłumacz tekst: {text} na język {language} w stylu {style}\" #dodanie 3 zmiennej\n",
    "prompt_3_variables = ChatPromptTemplate.from_template(template_string_1)\n",
    "\n",
    "prompt_3_variables.messages[0].prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00066340",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_message = ChatPromptTemplate.from_template(template_string)\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "customer_message = customer_message.format_messages(style=customer_style, text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b0ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(customer_message[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efc6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response = chat.invoke(customer_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd73616",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31951101",
   "metadata": {},
   "source": [
    "Nastepny przyklad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb99570",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939b20d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b71a3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "service_messages = prompt.format_messages(style=service_style_pirate, text=service_reply)\n",
    "service_response = chat.invoke(service_messages)\n",
    "service_response.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70be618",
   "metadata": {},
   "source": [
    "Krótkie powtorzenie do tego etapu:\n",
    "\n",
    "1. Defincja szablonu \n",
    "Zamiast pisac gotowy tekst, tworzymy formularz z miejscami na zmienne w nawiacahc klamrowych\n",
    "- Cel: oddzielenie instrukcji od danych wejsciowych\n",
    "- Kod `template_string = \"Przeltumacz {text} na styl {style}`\n",
    "\n",
    "2. Utworzenie obiektu promptu\n",
    "- Uzycie klasy `ChatPromptTemplate` aby zamienic suroy tekst w inteligetnych obiekt LangChain\n",
    "- Cel: przygotowanie struktury, ktora rozumieja modele typu Chat\n",
    "- Kod: `prompt_template = ChatPromptTemplate.from_template(template_string)`\n",
    "\n",
    "3. Formatowanie - wstrzykuejsz konkretne wartosci w miejscach zmiennych. Najlepiej uzyc metody `format_messages`\n",
    "- Cel: zamiana formularza na liste konkretnych wiadomosci np. `HumanMessage` ktore sa gotowe do wyslania\n",
    "- Kod: `formatted_prompt = prompt_template.format_messages(sty;e=customer_style, text=customer_email)`\n",
    "\n",
    "4. Wywolanie modelu invoke\n",
    "- Kod: `response = chat.invoke(formatted_prompt)`\n",
    "\n",
    "5. Wyciagniecie tresci\n",
    "- Kod `print(response.content)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71c87ff",
   "metadata": {},
   "source": [
    "### Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56f788a",
   "metadata": {},
   "source": [
    "Jak powinnien wygladac output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de724bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    \"gift\": False,\n",
    "    \"delivery_days\": 5,\n",
    "    \"price_value\": \"pretty affordable!\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d8008",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "delivery_days: How many days did it take for the product \\\n",
    "to arrive? If this information is not found, output -1.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "Format the output as JSON with the following keys:\n",
    "gift\n",
    "delivery_days\n",
    "price_value\n",
    "\n",
    "text: {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d156355",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_for_parser = ChatPromptTemplate.from_template(review_template)\n",
    "messages = prompt_template_for_parser.format_messages(text=customer_review)\n",
    "chat = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0.0)\n",
    "response = chat.invoke(messages)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a6ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e738349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#przy lini response.content.get() dostaniemy error ze wzgledu ze tos tr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0173ff90",
   "metadata": {},
   "source": [
    "### Parse the LLM output string into a Python dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8187db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
